[
  {
    "objectID": "notebooks/0_Basics.html",
    "href": "notebooks/0_Basics.html",
    "title": "Geopython basics",
    "section": "",
    "text": "This is a whirlwind tour of the basics of Python. In this notebook I will introduce you to the main data types you will encounter in Python, a few of the main packages you will use to manipulate data and the basics of making plots. There are tons of resources for learning the basics of Python, so if you want to know more - Youtube, ChatGPT, and Google are your friend. You should be able to follow this even if you have little to no programming experience, but I have created this with R users looking to get to grips with Python in mind"
  },
  {
    "objectID": "notebooks/0_Basics.html#arithmetic-operations",
    "href": "notebooks/0_Basics.html#arithmetic-operations",
    "title": "Geopython basics",
    "section": "Arithmetic operations",
    "text": "Arithmetic operations\nPython supports the usual arithmetic operators: + (addition), * (multiplication), / (division), ** (power), // (integer division)."
  },
  {
    "objectID": "notebooks/0_Basics.html#lists",
    "href": "notebooks/0_Basics.html#lists",
    "title": "Geopython basics",
    "section": "Lists",
    "text": "Lists\nLists are a container type for ordered sequences of elements. They are declared inside square brackets []. Lists can be initialized empty\n\nmy_list = []\n\nor with some initial elements\n\nmy_list = [1, 2, 3]\n\nLists have a dynamic size and elements can be added (appended) to them\n\nmy_list.append(4)\nmy_list\n\nWe can access individual elements of a list (indexing starts from 0)\n\nmy_list[2]\n\nWe can access “slices” of a list using my_list[i:j] where i is the start of the slice (again, indexing starts from 0) and j the end of the slice. For instance:\n\nmy_list[0:2]\n\nOmitting the second index means that the slice shoud run until the end of the list\n\nmy_list[1:]\n\nWe can check if an element is in the list using in\n\n5 in my_list\n\nThe length of a list can be obtained using the len function\n\nlen(my_list)"
  },
  {
    "objectID": "notebooks/0_Basics.html#strings",
    "href": "notebooks/0_Basics.html#strings",
    "title": "Geopython basics",
    "section": "Strings",
    "text": "Strings\nStrings are used to store text. They can delimited using either single quotes or double quotes\n\nstring1 = \"some text\"\nstring2 = 'some other text'\n\nStrings behave similarly to lists. As such we can access individual elements in exactly the same way\n\nstring1[3]\n\nand similarly for slices\n\nstring1[5:]\n\nString concatenation is performed using the + operator\n\nstring1 + \" \" + string2\n\nf-strings are a convinient way to insert variables into strings\n\nf'this is the list I created earlier {my_list}. neat'"
  },
  {
    "objectID": "notebooks/0_Basics.html#dictionaries",
    "href": "notebooks/0_Basics.html#dictionaries",
    "title": "Geopython basics",
    "section": "Dictionaries",
    "text": "Dictionaries\nIn Python, a dictionary is a built-in data structure that stores a collection of key-value pairs. Dictionaries are indexed by unique keys. Keys and values can be strings or numeric data types. Dictionaries are declared in curly brackets {}\nHere’s a basic example of a dictionary in Python:\n\n# Creating a dictionary\nstudent = {\n    \"name\": \"John\",\n    \"age\": 20,\n    \"degree\": \"B.Sc\"\n}\n\n# Accessing values using keys\nprint(student[\"name\"])\nprint(student[\"age\"])"
  },
  {
    "objectID": "notebooks/0_Basics.html#conditionals",
    "href": "notebooks/0_Basics.html#conditionals",
    "title": "Geopython basics",
    "section": "Conditionals",
    "text": "Conditionals\nAs their name indicates, conditionals are a way to execute code depending on whether a condition is True or False. As in other languages, Python supports if and else but else if is contracted into elif, as the example below demonstrates.\n\nmy_variable = 5\nif my_variable &lt; 0:\n  print(\"negative\")\nelif my_variable == 0:\n  print(\"null\")\nelse: # my_variable &gt; 0\n  print(\"positive\")\n\nHere &lt; and &gt; are the strict less and greater than operators, while == is the equality operator (not to be confused with =, the variable assignment operator). The operators &lt;= and &gt;= can be used for less (resp. greater) than or equal comparisons.\nContrary to other languages, blocks of code are delimited using indentation. Here, we use 2-space indentation but many programmers also use 4-space indentation. Any one is fine as long as you are consistent throughout your code."
  },
  {
    "objectID": "notebooks/0_Basics.html#loops",
    "href": "notebooks/0_Basics.html#loops",
    "title": "Geopython basics",
    "section": "Loops",
    "text": "Loops\nLoops are a way to execute a block of code multiple times. There are two main types of loops: while loops and for loops.\nWhile loop\n\ni = 0\nwhile i &lt; len(my_list):\n  print(my_list[i])\n  i += 1 # equivalent to i = i + 1\n\nFor loop\n\nfor i in range(len(my_list)):\n  print(my_list[i])\n\nIf the goal is simply to iterate over a list, we can do so directly as follows\n\nfor element in my_list:\n  print(element)"
  },
  {
    "objectID": "notebooks/0_Basics.html#functions",
    "href": "notebooks/0_Basics.html#functions",
    "title": "Geopython basics",
    "section": "Functions",
    "text": "Functions\nTo improve code readability, it is common to separate the code into different blocks, responsible for performing precise actions: functions. A function takes some inputs and process them to return some outputs.\n\ndef square(x):\n  return x ** 2\n\ndef multiply(a, b):\n  return a * b\n\n# Functions can be composed.\nsquare(multiply(3, 2))\n\nTo improve code readability, it is sometimes useful to explicitly name the arguments\n\nsquare(multiply(a=3, b=2))"
  },
  {
    "objectID": "notebooks/0_Basics.html#classes-and-object-oriented-programming",
    "href": "notebooks/0_Basics.html#classes-and-object-oriented-programming",
    "title": "Geopython basics",
    "section": "Classes and Object Oriented Programming",
    "text": "Classes and Object Oriented Programming\nSomething that you may not have encountered yet is the concept of Object Oriented Programming. Object-Oriented Programming (OOP) is a way to write programs using “objects”, which bundle related data and operations.\nClasses: Think of them as blueprints for creating similar objects. An example might be a Person class or a Dog class\nObjects: These are individual instances of a class. They have specific data (like a person’s name) and can perform operations (like a person walking). An example would be Glenn - an object of the Person class, or Murray - an object of the dog class\nIn OOP, you create instances from classes, similar to making houses from blueprints. When we define the class we can give it attributes and methods.\nLet see what this looks like in concete term by defing a dog class\n\nclass Dog\n\n    #__init__ is run when we create an object of this class\n    # here we can pass variables when creating an object after the mandatory 'self' argument\n    def __init__(self, name):\n        self.name = name  # set an attribute using self.attribute\n    \n    # create a method that belongs to this class using def\n    # the first argument must be self\n    def bark(self):     \n        return \"Woof!\"\n\n\n# Now, let's create an instance of the class Dog\n# This runs the __init__ function\nmy_dog = Dog(\"Murray\")\n\nThe my_dog instance has a single attribute - his name, Murray (a fine name for a dog). We set this in the __init__ method using self.name =. We can access the attributes of an instance using the pattern object.attribute\n\nmy_dog.name\n\nmy_dog has a single method - bark(). We can access a class’ method using the pattern instance.method(). Methods and functions are the same thing. A method is just a function that belongs to an instance\n\nmy_dog.bark()\n\n\n\n\nScreenshot 2023-06-30 at 01.47.09.png"
  },
  {
    "objectID": "notebooks/0_Basics.html#importing-packages",
    "href": "notebooks/0_Basics.html#importing-packages",
    "title": "Geopython basics",
    "section": "Importing packages",
    "text": "Importing packages\nA Python package is a collection of related Python scripts, called modules. Modules usually contain functions and classes that we will reuse.\nFor example, import math lets you use functions and classes in the math module, like math.sqrt(4) to get the square root of 4.\n\nimport math\n\nmath.sqrt(4)\n\nYou can import specific items from a module with from ... import ..., like from math import sqrt and then use sqrt(4) directly.\n\nfrom math import sqrt\nsqrt(4)\n\nYou can also nickname modules when importing to make them quicker to type, like import numpy as np. Now you can use numpy’s functions with the prefix np. instead of numpy.\nYou can import all the functions in module using from ... import *, like from math import *, similar to how things are done in R. This is strongly discouraged in Python\n\n#dont do this\nfrom math import *\nsqrt(4)\n\n\n#I told you not to do that!"
  },
  {
    "objectID": "notebooks/0_Basics.html#array-creation",
    "href": "notebooks/0_Basics.html#array-creation",
    "title": "Geopython basics",
    "section": "Array creation",
    "text": "Array creation\nNumPy arrays can be created from Python lists\n\nmy_array = np.array([1, 2, 3])\nmy_array\n\nNumPy supports arrays of arbitrary dimension. For example, we can create two-dimensional arrays (e.g. to store a matrix) as follows\n\nmy_2d_array = np.array([[1, 2, 3], [4, 5, 6]])\nmy_2d_array\n\nWe can access individual elements of a 2d-array using two indices as indexing works just like for Python lists\n\n# index using:\n# position_in_dim1, position_in_dim2\nmy_2d_array[1, 2]\n\nWe can also access rows\n\nmy_2d_array[1]\n\nand columns\n\nmy_2d_array[:, 2]\n\na bare index of : means give me everything in that dim\n\nprint(my_2d_array[:, :])\n#is the same as\nprint(my_2d_array)\n\nArrays have a shape attribute\n\nprint(my_array.shape)\nprint(my_2d_array.shape)\n\nContrary to Python lists, NumPy arrays must have a type and all elements of the array must have the same type.\n\nmy_array.dtype\n\nThe main types are int32 (32-bit integers), int64 (64-bit integers), float32 (32-bit real values) and float64 (64-bit real values).\nThe dtype can be specified when creating the array\n\nmy_array = np.array([1, 2, 3], dtype=np.float64)\nmy_array.dtype\n\nWe can create arrays of all zeros using\n\nzero_array = np.zeros((2, 3))\nzero_array\n\nand similarly for all ones using ones instead of zeros.\nWe can create a range of values using\n\nnp.arange(5)\n\nor specifying the starting point\n\nnp.arange(3, 5)\n\nAnother important operation is reshape, for changing the shape of an array\n\nmy_array = np.array([1, 2, 3, 4, 5, 6])\nmy_array.reshape(3, 2)\n\nPlay with these operations and make sure you understand them well."
  },
  {
    "objectID": "notebooks/0_Basics.html#basic-operations",
    "href": "notebooks/0_Basics.html#basic-operations",
    "title": "Geopython basics",
    "section": "Basic operations",
    "text": "Basic operations\nIn NumPy, we express computations directly over arrays. This makes the code much more succint.\nArithmetic operations can be performed directly over arrays. For instance, assuming two arrays have a compatible shape, we can add them as follows\n\narray_a = np.array([1, 2, 3])\narray_b = np.array([4, 5, 6])\narray_a + array_b\n\nNumPy has a range of functions that can be applied to arrays, e.g. np.sin\n\nnp.sin(array_a)\n\nVector inner product can be performed using np.dot\n\nnp.dot(array_a, array_b)\n\nnumpy arrays also have their own methods e.g matrix transpose .transpose()\n\narray_a.transpose()"
  },
  {
    "objectID": "notebooks/0_Basics.html#slicing-and-masking",
    "href": "notebooks/0_Basics.html#slicing-and-masking",
    "title": "Geopython basics",
    "section": "Slicing and masking",
    "text": "Slicing and masking\nLike Python lists, NumPy arrays support slicing\n\nz = np.arange(10)\n# you can read this as:\n# start at the 5th postion and give me everything from there until the end of that dim\nprint(z[5:])\n# you can read this as:\n# start at the beggining and give me everything until the 5th position\nprint(z[:5])\n\nWe can also select only certain elements from the array\n\nx = np.arange(10)\nmask = x &gt;= 5\nx[mask]\n\nBecause numpy arrays can have many dimensions, indexing can get quite complicated. If you want to know more have a look at the docs here. Lets look at what this looks like on a 3D array\n\n# Create a 3D array with values ranging from 0 to 269 with shape 3x3x30.\narray_3d = np.arange(270).reshape(3, 3, 30)\narray_3d\n\nLets pretend that this array has dimension latitude, longitude and band (3x3x30). Now lets extract a spatial subset - we only want the first 2x2 lat long grid with all wavelengths\n\n#dimensions separated by comma\n#1st dim - :2 indicates we want everything up to the 2nd coordinate i.e 0 and 1. Writing 0:2 would be equivalent\n#2nd dim - sanme as above\n#3rd dim -  : a bare semicolon indicate we want all entires in that dimension\nsliced_array = array_3d[:2, :2, :]\nsliced_array\n\nTo confirm our array has the correct shape (we expect 2x2x300) we can check the array’s shape attribute\n\nsliced_array.shape"
  },
  {
    "objectID": "notebooks/1_ML_ANG.html#overview",
    "href": "notebooks/1_ML_ANG.html#overview",
    "title": "Geospatial Machine learning",
    "section": "### Overview",
    "text": "### Overview\nIn this notebook, we will use existing data of verified land cover and alien species locations to extract spectra from AVIRIS NG surface reflectance data.\n\nLearning Objectives\n\n\nUnderstand how to inspect and prepare data for machine learning models\nTrain and interpret a machine learning model\nApply a trained model to AVIRIS imagery to create alien species maps\n\n\nRequirements\nThis tutorial requires the following Python modules installed s3fs, rioxarray, …\n\n\n\nLoad python modules\n\nimport geopandas as gpd\nimport xarray as xr\nfrom shapely.geometry import box, mapping\nimport rioxarray as riox\nimport numpy as np\nimport hvplot.xarray\nimport holoviews as hv\nimport xvec\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom dask.diagnostics import ProgressBar\n\ncustom functions\n\n#processes a multi-file datase\n#to extract the first valid reflectance measurement for each geometry. \n#iterates through the data, identifying the first non-null entry for each geometry across all files, \n#and then selects those specific reflectance values. \n\ndef get_first_xr(ds_in):\n    #array with geomtery x files\n    arr = ds_in['index'].data\n    \n    #we want to find the first file where the geom is not null\n    # Initialize an array to store the indices of the first non-null entry for each column\n    file_ind = np.full(arr.shape[1], -1)  # Using -1 as a placeholder for no non-null values\n    \n    # Iterate over each column\n    for col_idx in range(arr.shape[1]):\n        # Get the column\n        col = arr[:, col_idx]\n        \n        # Find the index of the first non-null entry\n        non_null_indices = np.where(~np.isnan(col))[0]\n        \n        if non_null_indices.size &gt; 0:\n            file_ind[col_idx] = non_null_indices[0]\n    \n    #create a list form 0 to len first_non_null_indices\n    geom_ind = list(range(len(file_ind)))\n    \n    file_ind = xr.DataArray(file_ind, dims=[\"index\"])\n    geom_ind = xr.DataArray(geom_ind, dims=[\"index\"])\n    \n    ds = ds_in['reflectance'][file_ind, geom_ind, :]\n    ds['index'] = ds['index'].astype(int)\n    raw_data_utm\n    #convert to dataset\n    ds = ds.to_dataset(name='reflectance')\n    \n    return ds\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\nhvplot.extension('bokeh')\n#hvplot.extension('matplotlib')\n%matplotlib inline\n\n\n\n1. Open and explore land cover labels\n\ntext_lab = ['Bare ground/Rock','Mature Fynbos','Recently burnt Fynbos','Wetland','Forest','Pine','Eucalyptus','Wattle','Water']\nlabel = ['0','1','2','3','4','5','6','7','8']\n\nlab_df = pd.DataFrame({\n    'class': label,\n    'text_lab': text_lab\n})\nlab_df\n\n\nraw_data = gpd.read_file('/home/gmoncrieff/ct_invasive.gpkg')\nraw_data_utm = (raw_data\n                .to_crs(\"EPSG:32734\")\n                .merge(lab_df, on='class', how='left')\n               )\nraw_data_utm.head()\n\n\n#explore data in interactive map. color by class. use google sattelite basemap\n(raw_data_utm[['text_lab','geometry']]\n .explore('text_lab',tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google'))\n\n\n\n2. Extract AVIRIS data at label locations\n\nAVNG_Coverage = gpd.read_file('ANG_Coverage.geojson')\n#filter dates to between midnight on 2023-11-09 and 23:59:59 on 2023-11-09\nAVNG_CP = AVNG_Coverage[(AVNG_Coverage['end_time'] &gt;= '2023-11-09 00:00:00') & (AVNG_Coverage['end_time'] &lt;= '2023-11-09 23:59:59')]\n#keep only AVNG_CP that intersects with raw_data\nAVNG_CP = AVNG_CP[AVNG_CP.intersects(raw_data.union_all())]\n\n#make a list of filenames\nfiles = AVNG_CP['RFL s3'].tolist()\nfiles.pop(70)\n\n#filter to start time between\n(AVNG_CP[['fid','geometry']]\n .explore('fid'))\n\n\n\ntest = xr.open_dataset(files[30], engine='kerchunk', chunks='auto')\ntest = test.where(test&gt;0)\ntest\n\n\ntest.sel(wavelength=[660, 570, 480], method=\"nearest\").hvplot.rgb('x', 'y',\n                                                                  rasterize=True,data_aspect=1,robust=True,\n                                                                  bands='wavelength',frame_width=400\n)\n\n\ntest.sel({'wavelength': 660},method='nearest').hvplot('x', 'y',\n                                                      rasterize=True, data_aspect=1,robust=True,\n                                                      cmap='magma',frame_width=400\n)\n\n\ndef extract_points(file,points):\n    \n    ds = xr.open_dataset(file, engine='kerchunk', chunks='auto')\n    \n    # Get the bounding box coordinates\n    left, bottom, right, top = ds.rio.bounds()\n    \n    # Create a Shapely box geometry\n    bbox_shapely = box(left, bottom, right, top)\n    \n    # Clip the raw data to the bounding box\n    points = points.clip(bbox_shapely)\n    print(f'got {points.shape[0]} point from {file}')\n    \n    # Extract points\n    extracted = ds.xvec.extract_points(points['geometry'], x_coords=\"x\", y_coords=\"y\",index=True)\n    \n    return extracted\n\n\nds_all = [extract_points(file,raw_data_utm) for file in files]\n\n\nds_all  = xr.concat(ds_all, dim='file')\nds_all\n\nextract the first valid reflectance measurement for each geometry\n\n\nds = get_first_xr(ds_all)\nds\n\nmerge with point data to add labels\n\nclass_xr =raw_data_utm[['class','group']].to_xarray()\nds = ds.merge(class_xr.astype(int),join='left')\nds\n\n\nwith ProgressBar():\n dsp = ds.persist()\n\n\ndsp\n\n\n\n3. Inspect AVIRIS spectra\n\ndsp_plot = dsp.where(dsp['class']==0, drop=True)\ndsp_plot['reflectance'].hvplot.line(x='wavelength',by='index',\n                                    color='green',ylim=(0,0.5),alpha=0.5,legend=False)\n\n\nedit data\n\n\n\n4. Prep data for ML model\n\nwavelengths_to_drop = ds.wavelength.where(\n    (ds.wavelength &lt; 420) |\n    (ds.wavelength &gt;= 1340) & (ds.wavelength &lt;= 1450) |\n    (ds.wavelength &gt;= 1800) & (ds.wavelength &lt;= 1980) |\n    (ds.wavelength &gt; 2400), drop=True\n)\n\n# Use drop_sel() to remove those specific wavelength ranges\ndsp = dsp.drop_sel(wavelength=wavelengths_to_drop)\n\n\n# Calculate the L2 norm along the 'wavelength' dimension in a Dask-aware way\nl2_norm = np.sqrt((dsp['reflectance'] ** 2).sum(dim='wavelength'))\n\n# Normalize the reflectance by dividing by the L2 norm\ndsp['reflectance'] = dsp['reflectance'] / l2_norm\n\n\ndsp_norm_plot = dsp.where(dsp['class']==4, drop=True)\ndsp_norm_plot['reflectance'].hvplot.line(x='wavelength',by='index',\n                                         color='green',ylim=(0,0.2),alpha=0.5,legend=False)\n\n\n\n5. Train and evaluate ML model\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n\n\ndtrain = dsp.where(dsp['group']==1,drop=True)\ndtest = dsp.where(dsp['group']==2,drop=True)\n\ny_train = dtrain['class'].values.astype(int)\ny_test = dtest['class'].values.astype(int)\nX_train = dtrain['reflectance'].values\nX_test = dtest['reflectance'].values\n\n\ndtrain = dsp.where(dsp['group']==1,drop=True)\ndtest = dsp.where(dsp['group']==2,drop=True)\n# Label encode the class variable\nle = LabelEncoder()\n\ny_train = le.fit_transform(dtrain['class'].values)\ny_test = le.transform(dtest['class'].values)\nX_train = dtrain['reflectance'].values\nX_test = dtest['reflectance'].values\n\n\n# Define the hyperparameter grid\nparam_grid = {\n    'n_estimators': [50, 250, 500],\n    'max_depth': [3, 5, 9],\n    'learning_rate': [0.1, 0.1, 0.001],\n    'subsample': [0.5, 1],\n    'min_child_weight': [1, 3, 5],\n    'gamma': [0, 0.1, 0.3]\n}\n\nparam_grid = {\n    'max_depth': [5],\n    'learning_rate': [0.1],\n    'subsample': [0.5, 1],\n    'n_estimators' : [50,100]\n}\n\n# Create the XGBoost model object\nxgb_model = xgb.XGBClassifier(tree_method='hist')\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object to the training data\ngrid_search.fit(X_train, y_train)\n\n# Print the best set of hyperparameters and the corresponding score\nprint(\"Best set of hyperparameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\nbest_model = grid_search.best_estimator_\n\n\ny_pred = best_model.predict(X_test)\n\n# Step 2: Calculate F1 score for the entire dataset\nf1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' accounts for class imbalance\nprint(f\"F1 Score (weighted): {f1}\")\n\n# Step 3: Calculate precision and recall for class 3\nprecision_class_3 = precision_score(y_test, y_pred, labels=[3], average='macro', zero_division=0)\nrecall_class_3 = recall_score(y_test, y_pred, labels=[3], average='macro', zero_division=0)\n\nprint(f\"Precision for Class 3: {precision_class_3}\")\nprint(f\"Recall for Class 3: {recall_class_3}\")\n\n# Step 4: Plot the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nConfusionMatrixDisplay(confusion_matrix=conf_matrix).plot()\n\n\nconformal\n\n\n\n6. Interpret and understand ML model\nSHAP scores\n\nimport shap\n#shap.initjs()\n\n# Initialize the SHAP Tree Explainer for XGBoost\nexplainer = shap.TreeExplainer(best_model,feature_names=list(map(str, dsp.wavelength.values.astype(int))))\nshap_values = explainer(X_test)\n\n\nsel_in = 5\nshap.plots.waterfall(shap_values[sel_in,:,y_test[sel_in]])\n\n\n#lables for wavelength in plot\n#feats =  dsp.wavelength.values.astype(int)\n#feats = map(str,feats)\n#shap.force_plot(explainer.expected_value[y_test[sel_in]], shap_values.values[sel_ind,:,y_test[sel_in]], pd.DataFrame(X_test).iloc[sel_ind, :],link='logit',feature_names=list(feats))\n\n\nshap.plots.beeswarm(shap_values[:,:,y_test[sel_in]].abs, color=\"shap_red\")\n\n\n#plot spectra and importance\nimportance = np.abs(shap_values[sel_in,:,y_test[sel_in]].values)\n# Create the base plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Get the wavelength and importance data\nwavelength = dsp['wavelength'].values\nimportance = importance  # Make sure this aligns with your wavelength data\n\n# Create a colormap\ncmap = plt.get_cmap('hot').reversed()  # You can choose a different colormap if you prefer\n\n# Normalize importance values to [0, 1] for colormap\nnorm = plt.Normalize(importance.min(), importance.max())\n\n# Add shading\nfor i in range(len(wavelength) - 1):\n    ax.fill_between([wavelength[i], wavelength[i+1]], 0, 1, \n                    color=cmap(norm(importance[i])), alpha=0.3)\n\n# Add a colorbar to show the importance scale\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\ncbar = plt.colorbar(sm, ax=ax, label='Importance')\n\n# Add white blocks to obscure specified regions\nax.fill_between([0,420], 0, 1, color='white')\nax.fill_between([1340,1458], 0, 1, color='white')\nax.fill_between([1800,1980], 0, 1, color='white')\nax.fill_between([2400,2500], 0, 1, color='white')\nax.set_xlim(420,2400)\n\nplot_xr = xr.DataArray(X_test[sel_in], coords=[wavelength], dims=[\"wavelength\"])\nplot_xr.plot.line(x='wavelength', color='green', ylim=(0, 0.2), ax=ax,zorder=0)\n\nplt.title('Reflectance with Importance Shading')\nplt.xlabel('Wavelength')\nplt.ylabel('Reflectance (normalized)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n7. Prep AVIRIS scenes or prediction\nLet me explain the key components of this solution:\npredict_on_chunk function:\nThis function is designed to work on individual chunks of your data. It ensures the input is a numpy array and reshapes it if necessary to match your model’s input requirements. It applies the model’s predict function to the chunk.\npredict_xarray function:\nThis function uses xarray’s apply_ufunc to apply the predict_on_chunk function across your entire dataset. It respects the chunking of your xarray DataArray and processes data chunk by chunk. The input_core_dims=[[‘wavelength’]] specifies that the ‘wavelength’ dimension should be passed to the function. output_core_dims=[[]] assumes that the prediction output is a single value per sample. Adjust this if your model outputs multiple values per sample. vectorize=True allows the function to work on arrays. dask=‘allowed’ enables parallel processing with dask.\nUsage:\nYou can apply this function to your xarray DataArray. The compute() call at the end triggers the actual computation and returns the results.\n\nSAPAD = (gpd.read_file('SAPAD_2024.gpkg')\n         .query(\"SITE_TYPE!='Marine Protected Area'\")\n        )\n# Get the bounding box of the first GeoDataFrame\nbbox = raw_data.total_bounds  # (minx, miny, maxx, maxy)\ngdf_bbox = gpd.GeoDataFrame({'geometry': [box(*bbox)]}, crs=raw_data.crs)  # Specify the CRS\ngdf_bbox['geometry'] = gdf_bbox.buffer(0.02)\n\n# Filter the second GeoDataFrame to keep only the rows that intersect with the buffered bbox\nSAPAD_CT = SAPAD.overlay(gdf_bbox,how='intersection')\n\n\nSAPAD_CT.explore()\n\n\n#keep only AVNG_CP that intersects with raw_data\nAVNG_sapad = AVNG_CP[AVNG_CP.intersects(SAPAD_CT.union_all())]\nfiles_sapad = AVNG_sapad['RFL s3'].tolist()\ngeometries_sapad = SAPAD_CT.to_crs(\"EPSG:32734\").geometry.apply(mapping)\n\n\n\n8. Predict over multiple AVIRIS scenes\n\ndef predict_proba_on_chunk(chunk, model):\n    probabilities = model.predict_proba(chunk)\n    return probabilities\n\n\n#files_s=files_sapad[62:70]\nfiles_s=files_sapad\n# Get the number of classes from a small prediction\nn_classes = 9\n\n\nfiles_s.pop(85)\nfiles_s.pop(87)\n\n\ndef predict_xr(file,geometries):\n    print(f'file: {file}')\n    ds = xr.open_dataset(file, engine='kerchunk', chunks='auto')\n    #condition to use for masking no data later\n    \n    condition = (ds['reflectance'] &gt; 0).any(dim='wavelength')\n    ds = ds.stack(sample=('x','y'))\n    \n    wavelengths_to_drop = ds.wavelength.where(\n        (ds.wavelength &lt; 420) |\n        (ds.wavelength &gt;= 1340) & (ds.wavelength &lt;= 1450) |\n        (ds.wavelength &gt;= 1800) & (ds.wavelength &lt;= 1980) |\n        (ds.wavelength &gt; 2400), drop=True\n    )\n    \n    # Use drop_sel() to remove those specific wavelength ranges\n    ds = ds.drop_sel(wavelength=wavelengths_to_drop)\n    \n    # Calculate the L2 norm along the 'wavelength' dimension in a Dask-aware way\n    l2_norm = np.sqrt((ds['reflectance'] ** 2).sum(dim='wavelength'))\n    \n    # Normalize the reflectance by dividing by the L2 norm\n    ds['reflectance'] = ds['reflectance'] / l2_norm\n    \n    \n    # Use apply_ufunc to apply the prediction function over chunks\n    result = xr.apply_ufunc(\n        predict_proba_on_chunk,\n        ds['reflectance'],\n        input_core_dims=[['wavelength']],#input dim with features\n        output_core_dims=[['class']],  # output dims for probabilities\n        exclude_dims=set(('wavelength',)),  #dims to drop in result\n        output_sizes={'class': n_classes},\n        output_dtypes=[np.float32],\n        dask=\"parallelized\",\n        kwargs={'model': best_model}\n    )\n    result = result.unstack('sample')\n    result = result.rio.set_spatial_dims(x_dim='x',y_dim='y')\n    result = result.rio.write_crs(\"EPSG:32734\")\n    result = result.rio.clip(geometries).where(condition)\n    result = result.transpose('class', 'y', 'x')\n    return result\n\n\ntest  = predict_xr(files_s[53],geometries_sapad)\ntest\n\n\ntest = test.rio.reproject(\"EPSG:4326\")\n\n\ntest.isel({'class':0}).hvplot(tiles=hv.element.tiles.EsriImagery(), \n                              project=True,rasterize=True,robust=True,\n                              cmap='magma',frame_width=400,data_aspect=1,alpha=0.5)\n\n\ngrid_pred = [predict_xr(fi,geometries_sapad) for fi in files_s]",
    "crumbs": [
      "Geospatial Machine learning"
    ]
  },
  {
    "objectID": "notebooks/1_ML_ANG.html#merge-and-mosaic-results",
    "href": "notebooks/1_ML_ANG.html#merge-and-mosaic-results",
    "title": "Geospatial Machine learning",
    "section": "9. Merge and mosaic results",
    "text": "9. Merge and mosaic results\n\nfrom rioxarray.merge import merge_arrays\n\n\nmerged = merge_arrays(grid_pred)\nmerged = merged.rio.reproject(\"EPSG:4326\")\nmerged.rio.to_raster('/home/gmoncrieff/ct_invasive.tiff',driver=\"COG\")\n\n\nmerged = xr.open_dataset('/home/gmoncrieff/ct_invasive.tiff', engine='rasterio', chunks='auto')\n\n\nmerged\n\n\nmerged.isel({'band':6}).hvplot(x='x',y='y',tiles=hv.element.tiles.EsriImagery(),\n                               geo=True,\n                                project=True,rasterize=True,robust=True,\n                                cmap='magma',clim=(0,1), frame_width=400,data_aspect=1,alpha=0.5)\n\n\nreturn to step 1",
    "crumbs": [
      "Geospatial Machine learning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TNC Science Gathering 2024 capacity building session: Cloud-based Geospatial Analysis",
    "section": "",
    "text": "Materials for cloud-based analysis of geospatial data capacity building session at TNC science gathering 2024\nClick the button below to open the Jupyter notebook in Studio Lab. Be sure to select ‘clone entire repository’ when prompted. Studio lab will also ask if you want to install required packages from environment.yml.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html",
    "href": "notebooks/1_Geopython.html",
    "title": "Intro to Geospatial Python",
    "section": "",
    "text": "A number of packages exist for working with geospatial data in Python. Some are more widely used than others. GeoPandas is the standard for working with spatial vector data. For working with raster data traditionally Rasterio has been the standard. Rasterio is a wrapper for GDAL which you may already be familiar with. Rasterio is not well suited to working with data with more than 2 dimensions (lat/long) or with a large number of bands. Xarray is well suited to high-dimensional data, and is rapidly growing in popularity. Hence this is what we will focus on. We will also spend a fair bit of time learning about dask, a package for making xarray (and Python in general) scale to large datasets.",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#geopandas-basics",
    "href": "notebooks/1_Geopython.html#geopandas-basics",
    "title": "Intro to Geospatial Python",
    "section": "1.0 - GeoPandas basics",
    "text": "1.0 - GeoPandas basics\nGeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types. If you have worked with sf in R, you will find Geopandas very familiar. Underneath, Geopandas uses GEOS for geometric calculcation via the shapely package. In R, sf also uses GEOS.\nThe core data structure in GeoPandas is the geopandas.GeoDataFrame, a subclass of pandas.DataFrame that can store geometry columns and perform spatial operations. A GeoDataFrame is a combination of regular pandas columns (pandas.Series), with traditional data and a special geometry column (geopandas.GeoSeries), with geometries (points, polygons, etc.). The geometry column has a GeoSeries.crs attribute, which stores information about the projection.\n\n\n\nScreenshot 2023-07-14 at 12.43.20.png\n\n\nIn this example, we will explore 2 datasets, swfynbos.gpkg, a dataset of the vegetation types of the southwestern Cape of South Africa, and fynbos_remnants, a dataset of the remaining fragments of natural vegetation in this region. This data is in the geopackage format but Geopandas can open all commonly encountered geospatial vector data formats.\n\n#we typically use the alias gpd\nimport geopandas as gpd\n\n#read file\nvegtypes = gpd.read_file('data/swfynbos.gpkg')\n\n#view some rows\nvegtypes.head()\n\nBefore getting into some data manipulations, lets looks at some attributes of the data. Geopandas allows us to easlity access some relevant attributes of our data\n\n#the type of each geometry\nprint(vegtypes.type)\n\n\n#area of each polygon\nprint(vegtypes.area)\n\n\n#centroid of each polygon\nprint(vegtypes.centroid)\n\nWe can print the coordinate reference system of the geodataframe using .crs\n\nvegtypes.crs\n\nDoing things like filters, selecting columns and rows, etc works exactly like a Pandas dataframe, as a geodataframe is a subclass of a dataframe\n\n#select first 5 rows\nprint(vegtypes.iloc[0:5])\n\n\n#filter to a single vegtypes\nprint(vegtypes.query('Name_18 == \"Hangklip Sand Fynbos\"').head())\n\nPlotting is easy too. Like Pandas there is a handy .plot() method for geodataframes. If we give it a column name as the first argument, it will color the plot by that column.\n\n#colour plot by vegetation type\nvegtypes.plot('Name_18')",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#geometric-manipulations-and-joins",
    "href": "notebooks/1_Geopython.html#geometric-manipulations-and-joins",
    "title": "Intro to Geospatial Python",
    "section": "1.1 - Geometric manipulations and joins",
    "text": "1.1 - Geometric manipulations and joins\nBefore we start playing with maniplating geodataframes based on their geometries, let’s load another dataset that we will combine with the first\n\nremnants = gpd.read_file('data/remnants.gpkg')\n#lets view the first few rows\nremnants.head()\n\nSet operations like intersections and unions can be applied using the gpd.overlay() function. Let’s extract the remaining natural vegetation of each vegetation type\n\n#intersection of vegtypes and remnants\nveg_remnants = gpd.overlay(vegtypes,remnants,how='intersection')\n\n#plot!\nveg_remnants.plot('Name_18')\n\nWhen executing set operations, the properties from both input dataframes are retained, so each row in the output will have all the columns from the inputs\n\nveg_remnants.head()\n\nfinally, lets combine all polygons with the same threat status together using dissolve to simplify our geodataframe\n\n#all polygons with the saem threat status into one\nveg_remnants_simple = veg_remnants.dissolve('RLE2021')\n#view\nveg_remnants_simple.head()\n\nThere is tons more functionality in GeoPandas, you can spatially join geodatframes with .sjoin(), reproject using to_crs(), and do all the good stuff you would expect. Two great places to dive deeper are the GeoPandas user guide, and the Carpentries lesson on vector data in Python",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#xarray-basics",
    "href": "notebooks/1_Geopython.html#xarray-basics",
    "title": "Intro to Geospatial Python",
    "section": "2.0 - Xarray basics",
    "text": "2.0 - Xarray basics\nXarray is the meat and potatoes of working with multidimensional gridded data in Python. While numpy provides many of the core operations we need for working with gridded data like indexing matrix operations, etc it does not provide the functionality to add information about the various dimensions of arrays, the coordinates of grid cells, or attached important metadata. This is where Xarray comes in.\nBy including labels on array dimensions Xarray opens up many new possibilities:\n\napplying operations over dimensions by name: x.sum(‘time’).\nselecting values by label x.sel(time=‘2014-01-01’).\nuse the split-apply-combine paradigm with groupby: x.groupby(‘time.dayofyear’).mean().\nkeeping track of arbitrary metadata in the form of a Python dictionary: x.attrs.\nand much more\n\nThe Xarray data structure makes it trivial to go from 2 to 3 to 4 to N dimensions, hence it is a great choice for working with imaging spectroscopy data where we will have at least 3 (lat, lon, wavelength) dimensions. Another big benefit is that it seamlessly integrates with Dask a popular library for parallel computing in Python. This allows us to scale analysis with Xarray to very large data.\nThe core data structure of Xarray is an xarray.DataArray - which in its simplest form is just a Numpy array with named dimensions and coordinates on those dimensions. We can combine multiple xarray.DataArray in a single structure called a xarray.Dataset. Let’s see what this looks like\n\n#typically we use the xr aliais\nimport xarray as xr\nimport numpy as np\n\n#create a 2x3 np array\narr = np.random.randn(2, 3)\n\n#create a xarray.DataArray by naming the dims and giving them coordinates\nxda = xr.DataArray(arr,\n                    dims=(\"x\", \"y\"),\n                    coords={\"x\": [10, 20],\n                            \"y\": [1.1,1.2,1.3]})\n\nxda\n\nWe can access the individual components like the data itself, the dimension names or the coordinates using accessors\n\n#get the actual data\nprint(xda.values)\n\n\n#get teh dimenson names\nprint(xda.dims)\n\n\n#get the x coordinates\nprint(xda.x)\n\nWe can set or get any metadata attribute we like\n\nxda.attrs[\"long_name\"] = \"random mesurement\"\nxda.attrs[\"random_attribute\"] = 123\n\nprint(xda.attrs)\n\nand perform calculations on xarray.DataArrays as if they were Numpy arrays\n\nxda + 10\n\n\nnp.sin(xda)\n\nAn xarray.Dataset is a container of multiple aligned DataArray objects\n\n#create a new dataarray with aligned dimensions (but it can be more or fewer dims)\n#create a new 2x3x4 xarray Dataarray\narr2 = np.random.randn(2, 3,4)\nxda2 = xr.DataArray(arr2,\n                    dims=(\"x\", \"y\",\"z\"),\n                    coords={\"x\": [10, 20],\n                            \"y\": [1.1,1.2,1.3],\n                            \"z\": [20,200,2000,20000]})\n\n#combine with another xarray.DataArray to make a xarray.Dataset\nxds = xr.Dataset({'foo':xda,'bar':xda2})\nxds\n\nHere you can see that we have multiple arrays in a single dataset. Xarray automatically aligns the arrays based on shared dimensions and coodrinates. You can do almost everything you can do with DataArray objects with Dataset objects (including indexing and arithmetic) if you prefer to work with multiple variables at once. You can also easily retrieve a single DataArray by name from a Dataset\n\nxds.foo\n# xds['foo'] works the same",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#terminology",
    "href": "notebooks/1_Geopython.html#terminology",
    "title": "Intro to Geospatial Python",
    "section": "2.1 - Terminology",
    "text": "2.1 - Terminology\nIt is important to be precise with our terminology when dealing with Xarrays as things can quickly get confusing when working with many dims. The full glossary can be found here, but a quick recap: - xarray.DataArray - A multi-dimensional array with labeled or named dimensions - xarray.Dataset - A collection of DataArrays with aligned dimensions - Dimension - The (named) axes of an array - Coordinate - An array that labels a dimension\n\n\n\nScreenshot 2023-07-14 at 20.57.13.png",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#loading-data-from-files",
    "href": "notebooks/1_Geopython.html#loading-data-from-files",
    "title": "Intro to Geospatial Python",
    "section": "2.2 - loading data from files",
    "text": "2.2 - loading data from files\nXarray supports reading and writing of several file formats, from simple Pickle files to the more flexible netCDF format. The recommended way to store Xarray data structures is netCDF. Xarray is based on the netCDF data model, so netCDF files on disk directly correspond to Dataset objects. If you aren’t familiar with this data format, the netCDF FAQ is a good place to start. When we are working with complex multidimensional data, file formats start to matter a lot, and they make a big difference to how fast and efficiently we can load and analyse data. More on this in the next lesson.\nWe can load netCDF files to create a new Dataset using open_dataset(). Similarly, a DataArray can be saved to disk using the DataArray.to_netcdf() method\nFor the rest of this lesson we will work with a small dataset from a Specim FENIX airbone imaging spectrometer collected near the town of Franschoek, near Cape Town in 2018. Lots of important metadata about the image has been removed to keep this simple. All that remains are the measured reflectances, and the latitude, longitude and wavelength coordinates.\n\nxda_is = xr.open_dataset(\"data/is_example.nc\")\nxda_is",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#indexing-selecting-and-masking",
    "href": "notebooks/1_Geopython.html#indexing-selecting-and-masking",
    "title": "Intro to Geospatial Python",
    "section": "2.3 - indexing, selecting and masking",
    "text": "2.3 - indexing, selecting and masking\nWhile you can use numpy-like indexing e.g da[:,:], this does not make use of the power of having named dims and coords. Xarray as specific method for selecting using the position in the array .isel() and using the coordinates with .sel()\n\n#idexing using position\nxda_is.isel(x=20,y=20)\n\nWe can extract a continous slice of an array dimension using slice()\n\nxda_is.isel(x=20,y=20,wl=slice(0,20))\n\nand provide a vector of indeces if we like\n\n#a list with the positions we want\nxind = [20,25,31]\n#select\nxda_is.isel(x=xind,y=20)\n\nWe can use all the same techniques, but provive coordinate values rather than positions if we use .sel(). We can also provide an option for what to do if we do not get an exact match to the provided coordinates.\n\nxda_is.sel(x=3.175e+05,y=6.263e+06,method='nearest')\n\nWe can mask values in our array using conditions based on the array values or coordinate values with .where()\n\n# drop bad bands\nxda_is = xda_is.where(xda_is.wl &lt; 2.1,drop=True)\nxda_is",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#grouping-and-aggregation",
    "href": "notebooks/1_Geopython.html#grouping-and-aggregation",
    "title": "Intro to Geospatial Python",
    "section": "2.4 - grouping and aggregation",
    "text": "2.4 - grouping and aggregation\nThe split-apply-combine approach of group-by + apply operations can be applied neatly using coordinates as grouping variables, or values with another xarray.DataArray. A simple application of this in imaging spectroscopy might be spectral resampling. Here we will group into 0.1µm or 100nm bins and calculate the mean reflectance per bin\n\n#first create a vector of the bins\nbins = np.arange(xda_is.wl.min(),xda_is.wl.max(),0.1)\n\n\nbins\n\n\n#groupby bin, then apply mean\nxda_bin = xda_is.groupby_bins(\"wl\", bins).mean()\nxda_bin\n\nYou may notice that often it takes almost no time at all to run xarray code. This is because for many functions xarray does not load data from disk and actually perform the calculation, rather it simply prints a summary and high-level overview of the data that will be produced. This is called Lazy computation and is the smart thing to do when working with large datasets. Only when you really need to do the calculation does it actually happen - like when calling .plot() or writing results. We can force computation by running xarray.DataArray.compute()\n\nxda_bin.compute()",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#plotting",
    "href": "notebooks/1_Geopython.html#plotting",
    "title": "Intro to Geospatial Python",
    "section": "2.5 - plotting",
    "text": "2.5 - plotting\nXarray is great for plotting. The simplest way to make a plot is to juse the .plot() method, much we like have deon with Pandas and GeoPandas. This will use matplotlib to create a plot for us. The type of plot will be determined from the number of dimensions in the Xarray.\n\n#plot a single pixel\n\n#select the pixel we want\npixel = xda_is.sel(x=3.175e+05,y=6.2625e+06,method='nearest')\n\n#make a scatter plot\npixel.reflectance.plot.scatter()\n\nIf we want to make a RGB plot, we can get back an image using plot.imshow(). This will automatically map the first 3 channels of the 3rd dim to Red Green Blue.\n\n#specify the wavelength of the RGB bands\nrgb=[0.66,0.56,0.48]\n\n\n#select the RGB bands\nrgbplot = xda_is.sel(wl=rgb,method='nearest')/10000\n#plot!\nrgbplot.reflectance.plot.imshow()\n\nThe abilities of the built-in .plot() method are somewhat limited. If you want to make beautiful, customized and interactive plots Holoviews and hvplot are the place to start. By importing holoview and hvplot, the .plot() method is replaced with .hvplot() and you plots instantly become way cooler.\n\nimport holoviews as hv\nimport hvplot.xarray\n\nLets make the same plot iof an individual pixel as we did before, but this time lets make it interactive by using hvplot\n\npixel.hvplot.scatter(y='reflectance',x='wl', color='black', frame_width=400)\n\nWe can replicate the rgb plot created using plot.imshow() with the hvplot.rgb() function\n\nrgbplot.reflectance.hvplot.rgb(x='x', y='y', bands='wl', aspect = 'equal', frame_width=400)\n\nbut the real magic happens when we start to customise the interactivity of our plots. With hvplot, it is relatvly straightforward to add thing like sliders and selectors. For example appling a groupby along a particular dimension allows us to explore the data as images along that dimension with a slider.\nThis provide a neat way for us to explore different wavelengths\n\nxda_is.reflectance.hvplot(x='x', y='y', groupby='wl', aspect = 'equal', frame_width=400, cmap = 'nipy_spectral',clim=(0,6000))\n\nIf you know what you are doing, you can build some pretty stunning visualization and enable rich exploration of data. Panel is a Python library built on top of holoviews that lets you easily build powerful tools, dashboards and complex applications entirely in Python. I recommend this Youtube video to get a sense of the amazing things that are possible with Geospatial data using these tools. Some examples here and here.",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#what-is-dask",
    "href": "notebooks/1_Geopython.html#what-is-dask",
    "title": "Intro to Geospatial Python",
    "section": "3.0 - What is dask",
    "text": "3.0 - What is dask\nXarray integrates with Dask to support parallel computations and streaming computation on datasets that don’t fit into memory. Currently, Dask is an entirely optional feature for Xarray. So you don’t need to use it if your data fit comfortably in memory and do not take an excessive amount of time to analyse. Dask is widely used to parallelize and scale workflows in Python, and is probably the most widely used package for this purpose. It is not just useful for working with Xarray, but rather builds on top of Python packages like Numpy and Pandas, and even more fundamental Python data structures.\nDask builds on top of Numpy by cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. This type of object - a chunked Numpy array - is called a dask.array\n\nimport dask.array as da\nimport numpy as np\n\n#create a big np array\ndata = np.arange(100_000).reshape(200, 500)\n\n#turn it into a dask.array by chopping it into chunks\ndarr1 = da.from_array(data, chunks=(100, 100))\ndarr1\n\nNow we have a 2D array with the shape (200, 500) composed of 10 chunks where each chunk has the shape (100, 100). Each chunk represents a piece of the data, and is just a normal np.array\nWe can execute most numpy functions on dask arrays. The difference is that they will not return the result, but rather just a reference to the yet-to-be initiated computation.\n\ndarr1.mean()\n\nRemember, to actual compute the results we need to call .compute(). This will return the result as a numpy array or list.\n\ndarr1.mean().compute()\n\nWe can force computation but keep the result in a chunked dask array by calling .persist(). This wont make much difference here, but we will see why this matters later.\nWe can chain operation together to create a more complex operation\n\ndarr2 = da.from_array(data*10, chunks=(100, 100))\ndarr3 = darr1 + darr2\ndarr4 = darr3.mean()\n\nAs we do this dask build a graph of all the computational steps that need to be performed to compute the desired results. When we call .compute() the graph is executed. We can visualize this graph\n\ndarr4.visualize(engine=\"cytoscape\")\n\nFor most analyses that we may encounter this graph will probably be very complex and can be challenging to interpret.",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#but-how-does-this-parallelize-work-and-allow-us-to-scale-to-very-large-datasets-you-ask",
    "href": "notebooks/1_Geopython.html#but-how-does-this-parallelize-work-and-allow-us-to-scale-to-very-large-datasets-you-ask",
    "title": "Intro to Geospatial Python",
    "section": "3.1 - But how does this parallelize work and allow us to scale to very large datasets you ask?",
    "text": "3.1 - But how does this parallelize work and allow us to scale to very large datasets you ask?\nWell, by splitting our array up into a collection of smaller chunks, and building a graph of operations to perform on these chunks we have split what might have originally been a single operation too large for our machine into many smaller operations. We can now pick as many of these operations as we like at a time to perform in parallel. We can also optimize the size of each chunk such that we get the right balance between one large chunk too big to fit in memory and many millions of chunks that would result in unmanageably large graphs.\nThe final piece of the puzzle is managing the workers that execute the units of work we have created, this is done by the Scheduler. Dask provides a few options to do this. The simplest way is the default and it happens in the background when you do a computation on a dask array on a single machine automatically.\nThe details of how this works are beyond the scope here, but here is how I think about it (I am probably wrong):\nThe scheduler creates a ‘pool’ of threads or processes to which it can allocate work (often n workers = n cores). A thread/process pool in Python is like a team of workers. You specify how many workers are on the team. You give the team a list of tasks. Each worker is assigned a task from the list and does it. If all workers are busy, the remaining tasks wait. When a worker finishes a task, it picks up a new one from the list. This way, tasks are done concurrently, but the number of workers doing the tasks doesn’t exceed the limit you set.\n\nThreads and Processes are not the same thing, and understanding the difference is very important if you want to properly understand parallel processing in Python. This is a complicated topic so I will not get into it here, but here is a good place to start. ChatGPT will also do a good job of explaining some of the concepts here.\n\nThe scheduler keeps track of all the tasks completed, running, and queued. It knows which tasks depend on which data, what data is available, and what data is waiting on what tasks to complete before it can be released.\n\n\n\nimage.png\n\n\nNow you have enough info to understand the image below which provides an overview of how dask works:\nOperations on Collections created a Task Graph. A task graph is then executed by a scheduler. We have seen the dask.array collection, but there are others like dask.dataframe. We have already worked with the default single-machine scheduler, but to scale to large datasets we need to use the Distributed Scheduler.\n\n\n\nimage.png",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#dask.distributed",
    "href": "notebooks/1_Geopython.html#dask.distributed",
    "title": "Intro to Geospatial Python",
    "section": "3.2 - Dask.distributed",
    "text": "3.2 - Dask.distributed\nThe distributed scheduler is more sophisticated, offers more features, and can run locally or distributed across a cluster. The downside is that to setup a distributed cluster with multiple worker machines takes a bit of work. Fortunately, we can run the distributed scheduler on a single without much setup, and we get a bunch of extra features that we don’t get with the default dask scheduler. However, for smaller data, the distributed scheduler will actually be much slower than the default scheduler or not using Dask at all. You should only use distributed when your data is much larger than what your computer can handle in memory.\nIf you want to find out about setting up a distributed schedular across a cluster you can read more here. Coiled is a great service that simplifies setting up a dask cluster across multiple cloud machines. With Coiled and Dask, you can quickly and easily get setup to analyse massive (Petabyte scale) datasets.\nFor the rest of this exercise we will use the distributed scheduler on a local machine, but all the features demoed will work the same if you use a cluster.\n\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers=4)  # Launches a distributed scheduler and workers locally\nclient = Client(cluster)  # Connect to distributed scheduler and override default\nclient\n\nclick on the dashboard link. It will open a tab but there won’t be much there yet. Let’s execute the computational graph we created earlier. Let’s make it a bit bigger so that we actually have some work to do\n\ndata = np.arange(20000000).reshape(2000, 10000)\ndarr1 = da.from_array(data, chunks=(100, 100))\ndarr2 = da.from_array(data*10, chunks=(100, 100))\ndarr3 = darr1 + darr2\ndarr4 = darr3.mean()\n\n\ndarr4.compute()\n\nNow go to the dashboard. There are numerous diagnostic plots available on the landing page.\n\nBytes Stored and Bytes per Worker:\nThese two plots show a summary of the overall memory usage on the cluster (Bytes Stored), as well as the individual usage on each worker (Bytes per Worker).\nTask Processing/CPU Utilization/Occupancy/Data Transfer:\nThe Processing tab in the figure shows the number of tasks that have been assigned to each worker. Not all of these tasks are necessarily executing at the moment: a worker only executes as many tasks at once as it has threads.\nProgress: The progress bars plot shows the progress of each individual task type. The color of each bar matches the color of the individual tasks on the task stream from the same task type\nTask Stream: The task stream is a view of which tasks have been running on each thread of each worker. It allows us to see not only the current activity but the past too. Each row represents a thread, and each rectangle represents an individual task. The color for each rectangle corresponds to the task type of the task being performed and matches the color of the Progress plot.\n\nIn some scenarios, the dashboard will have white spaces between each rectangle. During that time, the worker thread was idle. Having too much white space is an indication of sub-optimal use of resources. Additionally, a lot of long red bars (transfers) can indicate a performance problem, due to anything from too large of chunk sizes, too complex of a graph, or even poor scheduling choices. The task stream is the quickest way to diagnose if your computation has any issues. The example below shows you what the task stream for a healthy and unhealthy computation might look like\nAn example of a healthy Task Stream, with little to no white space.\n\n\n\ndashboard_taskstream_healthy copy.jpg\n\n\nAn example of an unhealthy Task Stream, with a lot of white space. Workers were idle most of the time. Additionally, there are some long transfers (red) which don’t overlap with computation.\n\n\n\ndashboard_task_stream_unhealthy copy.jpg\n\n\nThis was just a quick tour, the dask dashboard is very powerful and the first place to look when you want to understand how your computation is progressing. Read more about it here or watch this great Youtube video by dask creator Matthew Rocklin",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#chunking-is-important",
    "href": "notebooks/1_Geopython.html#chunking-is-important",
    "title": "Intro to Geospatial Python",
    "section": "Chunking is important",
    "text": "Chunking is important\nThe chunks parameter has critical performance implications when using Dask arrays. If your chunks are too small, queueing up operations will be extremely slow, because Dask will translate each operation into a huge number of operations mapped across chunks. Computation on Dask arrays with small chunks can also be slow, because each operation on a chunk has some fixed overhead from the Python interpreter and the Dask task executor.\nConversely, if your chunks are too big, some of your computation may be wasted if you don’t have workers but not enough chunks to keep them busy. You may also run out of memory with big chunks.\nA good rule of thumb is to create arrays with a minimum chunksize of at least one million elements (e.g., a 1000x1000 matrix). With large arrays (10+ GB), the cost of queueing up Dask operations can be noticeable, and you may need even larger chunksizes.\nCheckout the dask documentation on chunks to find out more about what to consider when deciding how to chunk your array.",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#automatic-parallelization-with-apply_ufunc",
    "href": "notebooks/1_Geopython.html#automatic-parallelization-with-apply_ufunc",
    "title": "Intro to Geospatial Python",
    "section": "3.4 - Automatic parallelization with apply_ufunc()",
    "text": "3.4 - Automatic parallelization with apply_ufunc()\nAlmost all of xarray’s built-in operations work on Dask arrays. If you want to use a function that isn’t wrapped by xarray, use apply_ufunc() to apply functions that consume and return Numpy arrays. apply_ufunc() will apply an embarrassingly parallel custom function to each chunk. The function will receive each chunk as a np.array and should return a np.array.\nLet’s look at a simple example. Normalisation is a common transformation applied to spectra to remove the influence of the absolute magnitude of reflectance and ensure that only the shape of spectra is relevant for subsequent analyses. After applying normalization all bands/wavelengths should sum to 1.\nHere is a function to normalise a np.array\n\ndef norm_wl(arr: np.array):\n    # Calculate the magnitude along the wavelength dimension\n    magnitude = np.sum(arr, axis=0, keepdims=True)\n    # Normalize the array along the wavelength dimension\n    normalized_array = arr / magnitude\n    return normalized_array\n\napply_ufunc() take the function we are mapping as it’s first arguemnt, then the xarray\n\nxda_norm = xr.apply_ufunc(\n    norm_wl,\n    xda_chunk['reflectance'],\n    dask = \"parallelized\",\n    output_dtypes=[int])\n\nxda_norm.persist()",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#rioxarray",
    "href": "notebooks/1_Geopython.html#rioxarray",
    "title": "Intro to Geospatial Python",
    "section": "4.0 - rioxarray",
    "text": "4.0 - rioxarray\nAlthough we have latitude and longitude values associated with our Xarray, this data is not a proper geospatial dataset and hence we cannot do spatial manipulations like calculating distances or reprojecting. Xarray is a general-purpose tool for any multidimensional data and is not specific to geospatial data. We need an additional package rioxarray which brings all of the power of GDAL to Xarrays. rioxarray extends Xarray with the rio accessor. What this means is that a bunch of new functions become available to Xarray instances by typing xarray.DataArray.rio.\n\nimport rioxarray\n\nThe first and most important detail we need to add before turning our Xarray into a geospatial dataset is information about the projection. Here we know the current crs is epsg:32734 (UTM zone 34S)\n\nxda_chunk = xda_chunk.rio.write_crs('epsg:32734')\nxda_chunk.rio.crs\n\n\nxda_chunk\n\nNow that we know the current projection, it is easy to reproject to match the projection of the vector data we were working with earlier\n\nxda_chunk_wgs = xda_chunk.rio.reproject('epsg:4326')\nxda_chunk_wgs\n\nNow that we are in the same projection as our vector data, let’s clip our raster to the vector data and extract only the intact natural vegetation of the vegetation type we are interested in\n\n#we need to do this first to deal with bad geometries in the vector data\nvclip = veg_remnants_simple.buffer(0)\n\n#clip\nxda_clip = xda_chunk_wgs.rio.clip(vclip)\n\nWe can repeat the plot we did earlier, but now only plotting data for intact natural vegetation\n\nxda_clip.hvplot(x='x', y='y', groupby='wl', aspect = 'equal', frame_width=400,clim=(0, 5000),cmap='nipy_spectral')\n\nrioxarray gives us the ability to read and write any file format supported by GDAL. This is as simple as rioxarray.open_rasterio() and xarray.DataArray.rio.to_raster()",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/1_Geopython.html#credits",
    "href": "notebooks/1_Geopython.html#credits",
    "title": "Intro to Geospatial Python",
    "section": "credits:",
    "text": "credits:\nThis lesson has borrowed heavily from:\nThe Carpentries Geospatial Python lesson by Ryan Avery\nThe geopandas user guide\nThe xarray user guide\nThe Dask tutorial from SciPy202\nAn Introduction to Earth and Environmental Data Science",
    "crumbs": [
      "Intro to Geospatial Python"
    ]
  },
  {
    "objectID": "notebooks/test.html",
    "href": "notebooks/test.html",
    "title": "My Document 3",
    "section": "",
    "text": "import math\nmath.pow(3,1)\n\n3.0",
    "crumbs": [
      "test"
    ]
  }
]